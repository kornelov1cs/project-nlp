{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e6e0166",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "# Natural Language Processing 2025-1A Homework 3\n",
    "\n",
    "# Vector Semantics, Word2Vec and LLMs \n",
    "\n",
    "Deadline: 1 October (23:59)\n",
    "\n",
    "Questions: Post them in the homework discussion on Canvas, sent them to nlp-course@utwente.nl or ask us during the practical sessions. \n",
    "\n",
    "How to submit: Please answer the questions directly in this notebook and submit it before the deadline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050ad9bd",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Please Write your group number, your names with student IDs Here: (double click here!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5705bb26",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Make sure that the following libraries are up-to-date in your computation envrionment. It is highly recommended to work on this assignment in UT's [JupyterLab](https://www.utwente.nl/en/service-portal/research-support/it-facilities-for-research/jupyterlab). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeaaa5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install --upgrade pip\n",
    "!pip3 install gensim nltk scikit-learn numpy pandas scipy \n",
    "!pip install  --upgrade gensim nltk scikit-learn numpy pandas scipy ### Upgrade your libraries if neccesary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc24ba63-c517-4ecf-bb9a-adce51f079b8",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "We'll need these libraries later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e976edb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas, numpy, scipy, math\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "from sklearn.metrics.pairwise import cosine_similarity \n",
    "from gensim.test.utils import datapath\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import codecs\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a13784d",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "In this assignment, you will first explore two types of word vectors: those generated using co-occurrence–based methods and those produced by the local-context predictive model Word2Vec. You will then apply and evaluate an NLP task powered by a Large Language Model (LLM). \n",
    "\n",
    "Note on Terminology: \n",
    "- The terms \"word\" and \"term\" are used interchangeably in this context, referring to unique tokens that you aim to represent as vectors. These tokens can be individual words, n-grams, phrases, or even identifiers, but for this assignment, we will focus on individual words.\n",
    "- Though \"word vectors\" and \"word embeddings\" are often used synonymously, they have distinct meanings.  According to [Wikipedia](https://en.wikipedia.org/wiki/Word_embedding), conceptually, word embedding \"*involves the mathematical embedding from space with many dimensions per word to a continuous vector space with a much lower dimension*\".\n",
    "\n",
    "# Part I. Co-occurrence count-based vectors\n",
    "\n",
    "Let's start with this corpus consisting of 10 sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0db5b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents=[\n",
    "    \"The warm sun melts the icy snow on the mountain.\",\n",
    "    \"A warm cup of tea felt perfect in the cool morning air.\",\n",
    "    \"Her warm smile brightened the cold winter day.\",\n",
    "    \"I love the contrast of a warm blanket on a cold night.\",\n",
    "    \"The cold wind chilled me, but the warm fire offered comfort.\",\n",
    "    \"After a cold swim, the warm towel felt like heaven.\",\n",
    "    \"The warm colors of the sunset clashed with the cold breeze.\",\n",
    "    \"The chilly floor left her longing for the cozy comfort of slippers.\",\n",
    "    \"A gentle breeze eased the bite of the cold ocean waves.\",\n",
    "    \"Cold hands found solace in the warm pockets of his coat.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e93419f",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Exercise 1.1.1 Construct the vocabulary (0.5 point)\n",
    "Before we construct co-occurrence matrices, we need to identify unique terms in the corpus, i.e. construct the vocabulary. You can remove stop words and apply other text normalisation operations before constructing the vocabulary. \n",
    "\n",
    "Tip: Sort your vocabulary alphabetically!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11e7e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect unique terms in the corpus\n",
    "\n",
    "# your code starts here\n",
    "\n",
    "\n",
    "\n",
    "# your code ends here\n",
    "\n",
    "print('The size of the vocabulary is', len(vocab))\n",
    "print('The word in the vocabulary are', vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb9245a",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Co-Occurrence\n",
    "\n",
    "A co-occurrence matrix counts how often terms co-occur in certain context. The context can be a complete document, a sentence, or a sliding window. \n",
    "\n",
    "Tip: Check out the [sklearn.feature_extraction.text](https://scikit-learn.org/stable/api/sklearn.feature_extraction.html#module-sklearn.feature_extraction.text) submodule that gathers utilities to build feature vectors from text documents. \n",
    "\n",
    "### Exercise 1.1.2 Term-document occurrence matrix and term-term co-occurrence matrix (0.5 point)\n",
    "Let's first consider **each sentence** in the above corpus to be the context where the (co-)occurrences are counted. For example, the words *warm*, *sun*, *icy* and *snow* occur in the first sentence, therefore, they occur in this document and co-occur with each other. Going through all the sentences, you can construct the term-document occurrence matrix and term-term co-occurrence matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111b9b50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Construct the term-document occurrence matrix\n",
    "\n",
    "# your code starts here\n",
    "\n",
    "\n",
    "# your code ends here\n",
    "\n",
    "print('The shape of the term-document matrix is', tdMatrix.shape)\n",
    "tdMatrix_pd = pandas.DataFrame(tdMatrix, index=vocab, columns=list(range(1, len(sents)+1)))\n",
    "tdMatrix_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980624f5",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "The term–term co-occurrence matrix can be computed directly from the term–document occurrence matrix. When doing so, pay close attention to the diagonal entries — they indicate self-co-occurrences, which may need to be removed or adjusted depending on your application. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374d9fe2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Construct the term-term co-occurrence matrix\n",
    "# Be sure to handle the diagonal elements appropriately\n",
    "\n",
    "# your code starts here\n",
    "\n",
    "\n",
    "\n",
    "# your code ends here\n",
    "print('The shape of the term-term matrix is', ttMatrix.shape)\n",
    "ttMatrix_pd = pandas.DataFrame(ttMatrix, index=vocab, columns=vocab)\n",
    "ttMatrix_pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d773caf",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Based on term-term co-occurrence matrix, which pair(s) of words co-occur the most? \n",
    "\n",
    "**YOUR ANSWER**: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba6b5a7",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Exercise 1.2 Cosine similarity\n",
    "The benefit of vector semantics is that the similarity of two words can be computed as the cosine similarity between their vectors. Let's now compare how similar two words are. \n",
    "\n",
    "### Exercise 1.2.1 Calculate cosine similarity between words (0.5 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bb97bc",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "What is the cosine similarity between \"cold\" and \"warm\" if 1) using term-document occurrence matrix 2) using term-term co-occurrence matrix?\n",
    "\n",
    "You may write your own cosine similarity function or use [`sklearn.metrics.pairwise.cosine_similarity`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html) to calculate the pair-wise cosine similarity among all the words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da533372-b700-4ed5-a8b4-44c070d69d51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate the cosine similarity between \"cold\" and \"ward\" using 1) using term-document occurrence matrix, \n",
    "# and 2) term-term co-occurrence matrix\n",
    "\n",
    "# your code starts here\n",
    "\n",
    "\n",
    "\n",
    "# your code ends here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c2ff1b",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Exercise 1.2.2 (0.5 point)\n",
    "\n",
    "Now we can calculate cosine similarity between words using a co-occurrence matrix. You can choose any previously constructed matrix for the similarity calculation. Rank all the words based on their similarity to the word *cold*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec803dab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Rank all the words by their similarity to word \"cold\"\n",
    "\n",
    "# your code starts here\n",
    "\n",
    "\n",
    "\n",
    "# your code ends here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06cc631",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "The calculated cosine similarity does not appear to capture semantic similarity or relatedness reliably. How might we obtain more meaningful similarity measures?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649c1612",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "**YOUR ANSWER**: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9221fd24",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "# Exercise 1.3 TF-IDF\n",
    "\n",
    "## Excercise 1.3.1 (0.5 point)\n",
    "For the above corpus, construct a TF-IDF weighted term-document matrix, using [`sklearn.feature_extraction.text.TfidfVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9342915-d753-4c46-a79c-3c12d7bc02b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Construct a TF-IDF weighted term-document matrix\n",
    "\n",
    "# your code starts here\n",
    "\n",
    "\n",
    "            \n",
    "# your code ends here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79440cf7",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Compute and rank the words in descending order based on their similarity to *cold*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57eb8349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and rank the words in descending order based on their similarity to *cold*\n",
    "\n",
    "# Your code starts here\n",
    "\n",
    "\n",
    "\n",
    "# Your code ends here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60efccf",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Exercise 1.3.2 (0.5 point)\n",
    "\n",
    "Let's use a bigger dataset which contains 2225 BBC news articles to construct TF-IDF term-document matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b725e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents=codecs.open('bbc-text.csv','r', encoding='utf-8').readlines() # load the data\n",
    "\n",
    "# your code starts here\n",
    "\n",
    "\n",
    "\n",
    "# your code ends here\n",
    "\n",
    "print('The size of the vocabulary is', len(vocab))\n",
    "print('The shape of the term-document matrix is', term_doc_matrix.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31793fb7",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "We can compute which words are most similar to *cold*. Does this list of words make more sense now and why?\n",
    "\n",
    "**YOUR ANSWER**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644c0120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the top 10 words that are most similar to word \"cold\"\n",
    "\n",
    "# your code starts here\n",
    "\n",
    "\n",
    "\n",
    "# your code ends here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5206eaef",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Find another 3 pairs of words whose cosine similarity makes sense to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51110f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for 3 pairs of words whose cosine similarities reflect their semantic similarity or relatedness.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbf1757",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "# Part II. Word2Vec word vectors\n",
    "\n",
    "Here, we explore the embeddings produced by word2vec. Please read J&M 6.8 or the [original paper](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) if you are interested in the details of the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fbd4ab",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Exercise 2.1 Pre-train word2vec model\n",
    "\n",
    "Run the following script to load the word2vec vectors into memory. **Note**: This might take several minutes. If you run out of memory, try closing other applicaions or restart your machine to free more memory. \n",
    "\n",
    "Please note, the following experiments run with Gensim 4.3.3. If you are still running an old version of Gensim, please upgrade your Gensim library or check [Migrating from Gensim 3.x to 4](https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4) to adapt your code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4ea97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 3 million Word2Vec Vectors, pre-trained on Google news, each with the dimension of 300\n",
    "# This model may take a few minutes to load.\n",
    "\n",
    "import gensim.downloader as api\n",
    "start_time = time.time()\n",
    "w2v_google = api.load(\"word2vec-google-news-300\")\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f880003",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loaded vocab size {}\".format(len(w2v_google.index_to_key)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedd53f8",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Once the model is loaded, you can extract the vector for individual words directly using `wv_google['']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67d4585",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_google['cold']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e956e1",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "One of the property of semantic embedding is that similar words are embedded close to each other. Use  `w2v_google.most_similar()` to identify the most similar words to *north*. Does this list make more sense to you? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20bcf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "for w,c in w2v_google.most_similar('cold'):\n",
    "    print(w,c)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e545c5",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Check a few more words to see whether their most similar words make sense to you and explain why. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed02633b",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_google.most_similar('black')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f427cb",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Word analogies\n",
    "\n",
    "An analogy explains one thing in terms of another to highlight the ways in which they are alike. For example, *paris* is similar to *france* in the same way that *rome* is to *italy*. Word2Vec vectors sometimes shows the ability of solving analogy problem of the form **a is to b as a* is to what?**.\n",
    "\n",
    "In the cell below, we show you how to use word vectors to find x. The `most_similar` function finds words that are most similar to the words in the `positive` list and most dissimilar from the words in the `negative` list. The answer to the analogy will be the word ranked most similar (largest numerical value). In the case below, the top one word *italy* is the answer, so this analogy is solved successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d152e4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to answer the analogy -- paris : france :: rome : x\n",
    "print(w2v_google.most_similar(positive=['rome', 'france'], negative=['paris']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757c654a",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Exercise 2.1.1 (0.5 point)\n",
    "Look for one analogy that can be solved successfully and one analogy that could not be solved using this pre-trained Word2Vec model. Check out [this paper](https://www.semanticscholar.org/paper/Efficient-Estimation-of-Word-Representations-in-Mikolov-Chen/330da625c15427c6e42ccfa3b747fb29e5835bf0) for inspirations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a55841e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your successful case goes here\n",
    "\n",
    "\n",
    "# Your failed case goes here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186fe0ba",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Visualising word analogies\n",
    "\n",
    "The following cell shows you how to use [tSNE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) to visualise a set of words based on their embeddings. You can also apply other dimensionality reduction methods (e.g. [sklearn.decomposition.TruncatedSVD](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html)) to reduce the vectors from 300-dimensional to 2 dimensional. \n",
    "\n",
    "Please note, reducing dimensionality from 300 to 2 is a very challenging task. You can try different parameters in the tSNE and see their effects on the final visualisation. In particular, the visualisation is very sensitive to the perplexity value that you give. Please try a few different perplexity valuse and keep the one that gives the most reasonable visusalisation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4515b4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def tsne_plot(model, wordlist, p):\n",
    "    labels = []\n",
    "    tokens = []\n",
    "    \n",
    "    for word in wordlist:\n",
    "        tokens.append(model[word])\n",
    "        labels.append(word)\n",
    "    \n",
    "    tokens = np.array(tokens)\n",
    "    \n",
    "    tsne_model = TSNE(perplexity=p, n_components=2, init='pca', max_iter=2500, random_state=23)\n",
    "    new_values = tsne_model.fit_transform(tokens)\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "        \n",
    "    plt.figure(figsize=(18, 18))\n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i], y[i])\n",
    "        plt.annotate(labels[i],\n",
    "                     xy=(x[i], y[i]),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "    plt.show()\n",
    "\n",
    "wordlist = ['man', 'woman', 'nephew', 'niece', 'brother', 'sister', 'uncle', 'aunt']\n",
    "tsne_plot(w2v_google, wordlist, len(wordlist) - 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dcd4c5",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Exercise 2.1.2 (0.5 point)\n",
    "Find another group analogies (at least 3 pairs of words) and see how they are visualised.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c486225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare at least 3 pairs of words   \n",
    "\n",
    "# your answer goes here\n",
    "\n",
    "wordlist=[]\n",
    "p=len(wordlist)-1\n",
    "tsne_plot(w2v_google,wordlist,p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1baa0b36",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Exercise 2.1.3  Synonyms and antonyms (0.5 point)\n",
    "\n",
    "\n",
    "\n",
    "Find three words (w1, w2, w3) so that \n",
    "- w1 and w2 are synonyms, \n",
    "- w1 and w3 are antonyms, \n",
    "- cosine_distance(w1, w2) > cosine_distance(w1, w3) or cosine_distance(w1, w2) $\\approx$ cosine_distance(w1, w3). \n",
    "\n",
    "Please give a possible explanation for why this has happened. \n",
    "\n",
    "You can use [`w2v_google.distance()`](https://radimrehurek.com/gensim/models/keyedvectors.html) function to compute the cosine distance between two words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b68367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace XXX, YYY and ZZZ with your chosen words\n",
    "\n",
    "w1='XXX'\n",
    "w2='YYY'\n",
    "w3='ZZZ'\n",
    "\n",
    "print(\"Synonyms {}, {} have cosine distance: {}\".format(w1, w2, w2v_google.distance(w1, w2)))\n",
    "print(\"Antonyms {}, {} have cosine distance: {}\".format(w1, w3, w2v_google.distance(w1, w3)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eee08b7",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "**Your answer**: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a78159",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Exercise 2.1.4 Polysemous Words (0.5 point)\n",
    "\n",
    "Some words are polysemous, i.e. they have multiple meanings. For example the word *bank* can be a financial institute or the rising ground bordering a lake or river. Find a polysemous word whose top most similar words contains related words from multiple meanings. You should use the the [`wv_google.most_similar()`](https://radimrehurek.com/gensim/models/keyedvectors.html) function to compute the closet neighbours of the word. You may increase the number of neighbours in order to identify multiple groups of meanings. Submit the ranked word list and explained how the words are grouped into different meanings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25cd2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_google.most_similar('crane',topn=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b39d280",
   "metadata": {},
   "outputs": [],
   "source": [
    "focusword='crane'\n",
    "wordlist=[focusword]\n",
    "for w in w2v_google.most_similar(focusword,topn=100):\n",
    "    wordlist.append(w[0])\n",
    "print(wordlist)\n",
    "tsne_plot(w2v_google,wordlist,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31efed6b",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Look into literature and describe potential methods to address this polysymy issue in word embeddings. Please cite the papers that you refer to. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfbab96",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "**YOUR ANSWER**:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18347b85",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Exercise 2.2  Self-trained Word2Vec model\n",
    "\n",
    "The word2vec model that we have been using so far is pre-trained on Google news. This is suitable for applications involving general topics. However, for special domains, such as scientific or medical domain, some domain-specific semantics could not be captured in the pre-trained model. Fortunately, word2vec is pretty efficient in training from scratch. We will use two different datasets to observer the effect on the input corpus. \n",
    "\n",
    "Importance parameters are highlighted in bold. Please choose a few different values and see their effects.  \n",
    "\n",
    "class gensim.models.word2vec.Word2Vec(sentences=None, corpus_file=None, **vector_size=100**, alpha=0.025, **window=5**, **min_count=5**, max_vocab_size=None, sample=0.001, seed=1, workers=3, min_alpha=0.0001, **sg=0**, hs=0, **negative=5**, ns_exponent=0.75, cbow_mean=1, hashfxn=<built-in function hash>, epochs=5, null_word=0, trim_rule=None, sorted_vocab=1, batch_words=10000, compute_loss=False, callbacks=(), comment=None, max_final_vocab=None, shrink_windows=True)\n",
    "    \n",
    "Please check the [gensim documentation](https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec) for more assistance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5cfb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the most similar words to 'young' in Google news\n",
    "w2v_google.most_similar('young')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874bc89c",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Exercise 2.2.1 (1 point)\n",
    "\n",
    "We first train a word2vec model on the corpus consisting the abstracts from 111K astrophysics/astronomy articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6497b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This might take up a few minutes to train.\n",
    "from gensim.models.word2vec import LineSentence, Word2Vec\n",
    "sentences=LineSentence('astro_norm.txt')\n",
    "\n",
    "start_time = time.time()\n",
    "# Train a word2vec model using the astro dataset\n",
    "# your code starts here\n",
    "\n",
    "\n",
    "\n",
    "# your code ends here\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19166f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_astro.wv.most_similar('young')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fc8a04",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "If all goes well, you may see *pms*, *proto* or *yso* among the top 10 most similar words to *young*. If you are curious, protostars and pre-main-sequence (PMS) stars are all [Young Stella Objects](https://en.wikipedia.org/wiki/Young_stellar_object)  (YSOs). Here, “young” means pre-main-sequence. For low-mass stars, this means ages of $10^5$ to $10^8$ years. [Ref](https://nexsci.caltech.edu/workshop/2003/2003_MSS/10_Thursday/mss2003_jensen.pdf)\n",
    "\n",
    "We then train a word2vec model on the corpus consisting of nearly 479K [Medline](https://www.nlm.nih.gov/medline/medline_overview.html) articles. Note, this corpus is rather big. If this is too much for your local machine, use UT's [JupyterLab](https://www.utwente.nl/en/service-portal/research-support/it-facilities-for-research/jupyterlab) or [Google Colab](https://colab.research.google.com/notebooks/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303b68c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This might take up half an hour to train!\n",
    "\n",
    "from gensim.models.word2vec import LineSentence, Word2Vec\n",
    "sentences=LineSentence('medline_norm.txt')\n",
    "\n",
    "start_time = time.time()\n",
    "# Train a word2vec model using the astro dataset\n",
    "# your code starts here\n",
    "\n",
    "\n",
    "\n",
    "# your code ends here\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6bb758",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_medline.wv.most_similar('young')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a40661b",
   "metadata": {},
   "source": [
    "Find another word and compute its most similar words based on different models. Please explain why this happens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb95a96",
   "metadata": {},
   "source": [
    "**YOUR ANSWER**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee509ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_google.most_similar('XXX')  # Replace XXX with your chosen word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892977b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_astro.wv.most_similar('XXX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e78314a",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_medline.wv.most_similar('XXX')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ba7781",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "**YOUR ANSWER to Why this happens:** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6c763b",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Exercise 2.2.2 (1 point)\n",
    "\n",
    "Experiment with different parameters, for example, the vector size, the window size, the minimal count, skip-gram or CBOW, etc. Observe their effects on the quality of the word embeddings and/or computational cost.\n",
    "\n",
    "You can apply intrinsic evaluations to compare the quality of your models. For example, your can check the correlation with human opinion on word similarity or on word analogies. Check gensim documentations for more options. For example, [evaluate_word_analogies](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.evaluate_word_analogies) and [evaluate_word_pairs](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.evaluate_word_pairs).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352924ca",
   "metadata": {},
   "source": [
    "**YOUR ANSWER**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6974261",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "**What are your observations?**\n",
    "\n",
    "**YOUR ANSWER**:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10896096-38be-47b6-acbf-454d651a9aa6",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "# Part III. Exploring and Evaluating a Large Language Model (LLM)\n",
    "\n",
    "In this part, you will apply what you’ve learned about representing and working with language to a modern NLP tool — a Large Language Model (LLM), — using free, local models via Hugging Face.\n",
    "You will design prompts, run them through an LLM, collect the outputs, and evaluate them critically.\n",
    "\n",
    "<div style=\"border: 3px solid #e67e22; padding: 15px; border-radius: 10px; background-color:#fff4e6; font-size: 16px;\">\n",
    "  <b>⚠️ Part III Instructions:</b>\n",
    "  You may either <b>keep your code and results directly in this Jupyter notebook</b>  \n",
    "  or <b>organise them in a separate document and upload it to Canvas</b>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4835eeb3-484b-4873-b21b-4869eca19eef",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Minimal environment setup (CPU, text‑only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9a769f-edf5-4606-99e4-2c5f9c494de6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade torch --index-url https://download.pytorch.org/whl/cpu\n",
    "!pip install --upgrade transformers pandas tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da23445c-7ee3-4347-9680-03ed87213768",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Set these before any transformers import to avoid TensorFlow/torchvision and accelerate vs. transformers collisions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf613525-bed0-4b14-adcf-1959151a4940",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, sys, site, subprocess\n",
    "\n",
    "# Block optional backends before *any* transformers import\n",
    "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\n",
    "os.environ[\"TRANSFORMERS_NO_TORCHVISION\"] = \"1\"\n",
    "os.environ[\"TRANSFORMERS_NO_JAX\"] = \"1\"\n",
    "\n",
    "# (Optional but helpful) install a clean CPU torch + stable transformers into your user site\n",
    "def pipi(args): subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--user\", \"--no-cache-dir\", \"--upgrade\"] + args)\n",
    "pipi([\"--index-url\", \"https://download.pytorch.org/whl/cpu\", \"torch==2.5.1+cpu\"])\n",
    "pipi([\"transformers==4.45.2\"])\n",
    "\n",
    "# Prefer user site-packages in this session\n",
    "user_site = site.getusersitepackages()\n",
    "if user_site not in sys.path:\n",
    "    sys.path.insert(0, user_site)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c106e745-e321-458f-8aa7-c0d3374e1292",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "## Warm-up Exercise: Playing with Small LLMs\n",
    "\n",
    "Before diving into the main assignment, try running one of these lightweight models locally. You’ll get a feel for how different architectures respond to prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5886b159-3fba-481b-8941-a533e3bca99c",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Option 1 – Seq2Seq Model (encoder-decoder)\n",
    "\n",
    "How it works: First encodes the input text into a hidden representation, then decodes it into an output sequence. The input and output can be different in length and form.\n",
    "\n",
    "Strengths: Great at transforming text from one form to another (e.g., summarization, translation, question answering).\n",
    "\n",
    "Examples: T5, BART, FLAN-T5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7efb90-7644-4928-a3e4-00f6900c1bd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# 1. Pick your model\n",
    "seq2seq_name = \"google/flan-t5-small\" # Options: \"google/flan-t5-small\" or \"facebook/bart-large-cnn\" \n",
    "\n",
    "# 2. Load with CPU optimization\n",
    "seq_tok = AutoTokenizer.from_pretrained(seq2seq_name)\n",
    "seq_model = AutoModelForSeq2SeqLM.from_pretrained(seq2seq_name)\n",
    "\n",
    "# 3. Prompt the model to provide an answer\n",
    "x = seq_tok(\"Write a haiku about AI\", return_tensors=\"pt\")\n",
    "y = seq_model.generate(**x, max_new_tokens=400, do_sample=True, temperature=0.8)\n",
    "print(seq_tok.decode(y[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61113a24-ff04-4fdb-aa5b-15bbe8684aac",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Option 2 - Causal Model (decoder-only)\n",
    "\n",
    "How it works: Predicts the next token in a sequence based only on the tokens before it, generating text step-by-step.\n",
    "\n",
    "Strengths: Ideal for free-form generation where you want the model to continue or expand on a prompt (e.g., storytelling, dialogue, creative writing).\n",
    "\n",
    "Examples: GPT-2, GPT-Neo, LLaMA, Mistral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325e250d-84ba-4a72-afa4-a707e6cfe1d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "causal_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # or \"microsoft/phi-2\", \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", \"distilgpt2\"\n",
    "causal_tok = AutoTokenizer.from_pretrained(causal_name)\n",
    "causal_model = AutoModelForCausalLM.from_pretrained(causal_name)\n",
    "\n",
    "prompt = \"Write a haiku about AI:\" # What happens if you remove the colon (:)?\n",
    "ids = causal_tok(prompt, return_tensors=\"pt\")\n",
    "out = causal_model.generate(\n",
    "    **ids,\n",
    "    max_new_tokens=80,\n",
    "    do_sample=True, temperature=0.8, top_p=0.95,\n",
    "    pad_token_id=causal_tok.eos_token_id or tok.pad_token_id  # important for some causal models\n",
    ")\n",
    "print(causal_tok.decode(out[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5a8bf6-3904-4b5c-94d7-fbf01c538e60",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "### Exercise 3.1 (1 point)\n",
    "\n",
    "Experiment with different prompt design techniques to explore how LLM responses vary, evaluate their effectiveness, and develop best practices for a chosen application scenario (e.g. summarisation, question answering, text classification, instruction following, etc.)\n",
    "\n",
    "Describe your NLP application and justify your choice of models - indicate whether you are using Seq2Seq (encoder–decoder) or Causal (decoder-only) models, explain why they are suited to your application, and mention any constraints (e.g., speed, resources, interpretability).\n",
    "\n",
    "**YOUR ANSWER:**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e221dae-26eb-4467-a0f1-899292db077f",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Exercise 3.2 (2 point)\n",
    "\n",
    "Try at least three different prompt techniques (zero-shot, few-shot, chain-of-thought, role prompting, etc.) and compare the results. Document the exact prompts text, the model's raw output, your reflection on effectiveness (e.g., factual accuracy, consistency, clarity, creativity, etc.)\n",
    "\n",
    "**YOUR ANSWER**:   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c3284c5-8579-4dbe-aed4-68c2967a37b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an example NLP scenario \n",
    "\n",
    "# Step 1: Define your task and prompt variants\n",
    "TASK = \"Write a short museum label (80-120 words) for Van Gogh's 'Sunflowers' that is engaging for teenagers.\"\n",
    "PROMPTS = [\n",
    "    {\"name\": \"zero-shot\", \"system\": \"You are a helpful museum guide.\", \"user\": TASK},\n",
    "    {\"name\": \"role+style\", \"system\": \"You are a witty museum educator for teenagers.\", \"user\": TASK + \" Use vivid, friendly language and one metaphor.\"},\n",
    "    {\"name\": \"cot-steps\", \"system\": \"You are a precise art historian.\", \"user\": \"Outline 3 key points (artist, context, significance) and then write the label. \" + TASK},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb4049d-61f1-48da-a017-62487ab89bdd",
   "metadata": {
    "user_expressions": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f400156-7725-4f43-a222-1bd5cbc83081",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
